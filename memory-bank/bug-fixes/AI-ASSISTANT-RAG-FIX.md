# AI Assistant + RAG Integration Fix

## Issues Found

### 1. ‚ùå Client-Side OpenAI API Key Error
**Error:**
```
AI_LoadAPIKeyError: OpenAI API key is missing.
Pass it using the 'apiKey' parameter or the OPENAI_API_KEY environment variable.
```

**Root Cause:**
- HybridAgent was routing "simple" queries to client-side agent
- Client-side agent tried to use OpenAI directly from React Native
- OpenAI API keys should NEVER be exposed client-side (security risk)

### 2. ‚ùå Incorrect Fallback Logic
```typescript
// Before: Fallback to client-side when server fails
if (!options.forceMode) {
  console.log('‚ö†Ô∏è [HybridAgent] Server failed, falling back to client-side');
  return await aiAgent.generateResponse(userMessage, conversationHistory);
}
```

**Problem:** Falls back to client-side (which also doesn't have API key) when server fails

### 3. ‚è±Ô∏è Timeout on Decisions Extraction
**Error:** `Timeout after 15s`

**Root Cause:**
- 15 seconds wasn't enough for 50 messages
- OpenAI API calls can take 10-20 seconds for large batches
- Image filtering adds processing time

---

## Solutions Applied

### ‚úÖ Fix 1: Always Use Server-Side (LangChain + RAG)

**Modified:** `services/ai/HybridAgent.ts`

**Before:**
```typescript
// Route to client-side for simple queries
if (complexity === 'simple' && !options.forceMode) {
  console.log('üì± [HybridAgent] Using client-side agent (fast)');
  return await aiAgent.generateResponse(userMessage, conversationHistory);
}
```

**After:**
```typescript
// Always route to server-side for security (no exposed API keys)
console.log('üî• [HybridAgent] Using server-side agent (LangChain + RAG + LangSmith)');

const knowledgeAgentFn = httpsCallable(functions, 'knowledgeAgent');
const result = await knowledgeAgentFn({
  question: userMessage,
  userId: options.userId,
  chatId: options.chatId,
  queryType: complexity === 'rag' || complexity === 'knowledge' ? 'rag' : 'general',
  conversationHistory: conversationHistory.slice(-5), // Last 5 messages for context
});
```

**Benefits:**
- ‚úÖ API keys never exposed to client
- ‚úÖ Full LangChain + RAG capabilities
- ‚úÖ LangSmith tracing for debugging
- ‚úÖ Pinecone vector search for knowledge retrieval

### ‚úÖ Fix 2: Removed Client-Side Fallback

**Before:**
```typescript
// Fallback to client-side if server fails
if (!options.forceMode) {
  console.log('‚ö†Ô∏è [HybridAgent] Server failed, falling back to client-side');
  return await aiAgent.generateResponse(userMessage, conversationHistory);
}
```

**After:**
```typescript
// Return helpful error message (no fallback)
throw new Error(
  'AI Assistant is temporarily unavailable. Please try again. ' +
  `(${error.code || error.message})`
);
```

**Benefits:**
- ‚úÖ Clear error messages for users
- ‚úÖ No confusing "fallback failed" errors
- ‚úÖ Encourages fixing server-side issues

### ‚úÖ Fix 3: Increased Timeout for Decisions

**Modified:** `app/(tabs)/decisions.tsx`

**Before:**
```typescript
// Add 15-second timeout per chat
const timeoutPromise = new Promise<never>((_, reject) => {
  setTimeout(() => reject(new Error('Timeout after 15s')), 15000);
});

const decisions = await Promise.race([
  trackDecisions(chat.id, 50),
  timeoutPromise
]);
```

**After:**
```typescript
// Add 30-second timeout per chat (OpenAI can take time with large message batches)
const timeoutPromise = new Promise<never>((_, reject) => {
  setTimeout(() => reject(new Error('Timeout after 30s')), 30000);
});

// Extract decisions (reduced to 30 messages for faster processing)
const decisions = await Promise.race([
  trackDecisions(chat.id, 30),
  timeoutPromise
]);
```

**Benefits:**
- ‚úÖ More time for OpenAI API calls
- ‚úÖ Fewer timeouts on large chats
- ‚úÖ Better user experience (fewer errors)

---

## Architecture Overview

### Server-Side AI Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         React Native Client                  ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚Ä¢ HybridAgent.ts                           ‚îÇ
‚îÇ  ‚Ä¢ Always routes to server                   ‚îÇ
‚îÇ  ‚Ä¢ No API keys exposed                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Firebase Functions
               ‚îÇ (HTTPS Callable)
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Firebase Functions (Server-Side)        ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚Ä¢ knowledgeAgent.ts                        ‚îÇ
‚îÇ  ‚Ä¢ LangChain (ChatOpenAI)                   ‚îÇ
‚îÇ  ‚Ä¢ LangSmith Tracing                        ‚îÇ
‚îÇ  ‚Ä¢ OpenAI API Key (secure)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îú‚îÄ‚îÄ‚îÄ Simple Queries ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ                               ‚îÇ
               ‚îÇ                               ‚ñº
               ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ                    ‚îÇ   OpenAI GPT-4o  ‚îÇ
               ‚îÇ                    ‚îÇ                  ‚îÇ
               ‚îÇ                    ‚îÇ   Direct answer  ‚îÇ
               ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ RAG Queries ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                               ‚îÇ
                                               ‚ñº
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ  Pinecone Vector  ‚îÇ
                                    ‚îÇ    Database       ‚îÇ
                                    ‚îÇ                   ‚îÇ
                                    ‚îÇ  ‚Ä¢ Embeddings     ‚îÇ
                                    ‚îÇ  ‚Ä¢ Messages       ‚îÇ
                                    ‚îÇ  ‚Ä¢ Summaries      ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ   OpenAI GPT-4o  ‚îÇ
                                    ‚îÇ                  ‚îÇ
                                    ‚îÇ  Context-aware   ‚îÇ
                                    ‚îÇ  RAG answer      ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Query Routing Logic

1. **User asks question** ‚Üí HybridAgent
2. **Assess complexity:**
   - `'simple'` - Direct OpenAI call
   - `'rag'` - Pinecone search + OpenAI
   - `'complex'` - LangChain reasoning + OpenAI
3. **Route to server-side:** `knowledgeAgent` Firebase Function
4. **LangSmith tracing:** Auto-enabled via env vars
5. **Return response:** Stream or complete answer

---

## LangChain + RAG Integration

### How RAG Works

**RAG = Retrieval-Augmented Generation**

1. **User asks:** "What did we decide about desserts?"
2. **Embed query:** Convert question to vector (OpenAI embeddings)
3. **Search Pinecone:** Find similar messages in vector database
4. **Retrieve context:** Get top 5 most relevant messages
5. **Generate answer:** Send context + question to GPT-4o
6. **Return answer:** "You decided to order cookies from Voodoo Dough"

### Code Implementation (knowledgeAgent.ts)

```typescript
// 1. Get vector store (Pinecone + OpenAI embeddings)
const store = await getVectorStore();

// 2. Perform similarity search
const results = await store.query(question, 5, { userId });

// 3. Build context from retrieved documents
const context = results.map(doc => doc.pageContent).join('\n\n---\n\n');

// 4. Generate response with context
const prompt = `You are an AI assistant for ChatIQ.
Use the following context from the user's messages to answer the question.

Context:
${context}

Question: ${question}`;

// 5. Call OpenAI with LangChain
const response = await model.invoke([{ role: 'user', content: prompt }]);
```

### LangSmith Tracing

**Auto-enabled via environment variables:**
```bash
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_key
LANGCHAIN_PROJECT=chatiq-rag
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
```

**What it tracks:**
- ‚úÖ All LangChain calls
- ‚úÖ Vector search queries
- ‚úÖ OpenAI API calls
- ‚úÖ Latency and errors
- ‚úÖ Token usage and costs

**View traces at:** https://smith.langchain.com

---

## Testing Instructions

### Test AI Assistant (RAG)

1. **Navigate to AI Assistant Tab**
   - Tap "AI Assistant" in bottom navigation

2. **Test Simple Query**
   - Ask: "What is ChatIQ?"
   - **Expected:**
     ```
     ü§ñ [HybridAgent] Query complexity: simple
     üî• [HybridAgent] Using server-side agent (LangChain + RAG + LangSmith)
     ```
   - Should respond quickly with general info

3. **Test RAG Query (Knowledge Retrieval)**
   - Ask: "What did we decide about desserts for Paul's birthday?"
   - **Expected:**
     ```
     ü§ñ [HybridAgent] Query complexity: rag
     üî• [HybridAgent] Using server-side agent (LangChain + RAG + LangSmith)
     üîç [KnowledgeAgent] Found 5 relevant documents
     ```
   - Should search Pinecone and return: "You decided to order cookies from Voodoo Dough"

4. **Test Complex Query**
   - Ask: "Analyze all my action items across all chats"
   - **Expected:** Uses LangChain reasoning with vector search

5. **Check Logs**
   - ‚úÖ No "OpenAI API key missing" errors
   - ‚úÖ All queries route to server-side
   - ‚úÖ LangSmith tracing enabled

### Test Decisions Tab (Timeout Fix)

1. **Navigate to Decisions Tab**
2. **Wait for auto-scan to complete**
3. **Expected:**
   - ‚úÖ No timeout errors
   - ‚úÖ Scans complete within 30 seconds
   - ‚úÖ Decisions extracted successfully

---

## Environment Variables Required

### Client-Side (.env)
```bash
# Firebase (public)
EXPO_PUBLIC_FIREBASE_API_KEY=...
EXPO_PUBLIC_FIREBASE_AUTH_DOMAIN=...
EXPO_PUBLIC_FIREBASE_PROJECT_ID=...

# Pinecone (public - index name only, no API key)
EXPO_PUBLIC_PINECONE_INDEX_NAME=chatiq-messages
```

### Server-Side (functions/.env)
```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Pinecone
EXPO_PUBLIC_PINECONE_API_KEY=...
EXPO_PUBLIC_PINECONE_INDEX=chatiq-messages

# LangSmith (for tracing)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=...
LANGCHAIN_PROJECT=chatiq-rag
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
```

**Security Note:**
- ‚úÖ API keys only in server-side `.env`
- ‚úÖ Never commit `.env` files
- ‚úÖ Use Firebase Functions config for production

---

## Deployment Status

### Functions Deployed
- ‚úÖ `knowledgeAgent` - LangChain + RAG + LangSmith
- ‚úÖ `searchVectorStore` - Pinecone search
- ‚úÖ `embedContent` - Vector embedding

### Client Updates
- ‚úÖ `HybridAgent.ts` - Always uses server-side
- ‚úÖ `decisions.tsx` - Increased timeout to 30s

### Ready for Testing
```bash
# App should already be reloaded
# Test AI Assistant feature now
```

---

## Performance Expectations

### AI Assistant Response Times

1. **Simple Queries** (No RAG)
   - Expected: 1-3 seconds
   - Example: "What is ChatIQ?"

2. **RAG Queries** (Vector Search + Generation)
   - Expected: 2-5 seconds
   - Example: "What did we decide about desserts?"

3. **Complex Queries** (LangChain Reasoning)
   - Expected: 3-7 seconds
   - Example: "Analyze my action items"

### Decisions Auto-Scan
- Expected: 10-30 seconds per chat
- Timeout: 30 seconds (prevents hangs)

---

## Troubleshooting

### If AI Assistant Still Fails

1. **Check Server-Side Logs**
   ```bash
   firebase functions:log | grep -i "knowledge"
   ```

2. **Verify knowledgeAgent Deployment**
   ```bash
   firebase functions:list | grep knowledgeAgent
   ```
   - Should show: `knowledgeAgent ‚îÇ v1 ‚îÇ callable`

3. **Check Environment Variables**
   ```bash
   cd functions
   cat .env | grep -E "OPENAI|PINECONE|LANGCHAIN"
   ```
   - Verify all keys are set

4. **Test Direct Function Call**
   ```bash
   # In Firebase Console Functions section
   # Test knowledgeAgent with:
   {
     "question": "Hello",
     "userId": "test",
     "queryType": "general"
   }
   ```

### If Decisions Timeout

1. **Reduce Message Limit**
   - Already reduced to 30 (from 50)
   - Can reduce further to 20 if needed

2. **Check OpenAI API Status**
   - https://status.openai.com

3. **Check Firebase Functions Logs**
   ```bash
   firebase functions:log | grep extractDecisions
   ```

---

## Summary

### What Was Fixed

1. ‚úÖ **AI Assistant now uses server-side LangChain + RAG**
   - No more OpenAI API key errors
   - Secure (no exposed keys)
   - Full RAG capabilities with Pinecone

2. ‚úÖ **LangSmith integration working**
   - Auto-tracing enabled
   - Monitor all queries at smith.langchain.com

3. ‚úÖ **Decisions timeout increased**
   - 15s ‚Üí 30s timeout
   - 50 ‚Üí 30 message limit
   - Fewer timeout errors

### Architecture Benefits

- üîê **Secure**: API keys never exposed to client
- üöÄ **Fast**: Optimized for React Native
- üß† **Smart**: RAG retrieval for context-aware answers
- üìä **Observable**: LangSmith tracing for debugging
- ‚ôªÔ∏è **Reusable**: Same server-side stack for all AI features

**Status:** ‚úÖ Ready for testing
